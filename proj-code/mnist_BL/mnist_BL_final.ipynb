{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd204f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▇▇████</td></tr><tr><td>train_f1_score_macro</td><td>▁▇█████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁</td></tr><tr><td>train_precision_macro</td><td>▁▇▇████</td></tr><tr><td>train_recall_macro</td><td>▁▇█████</td></tr><tr><td>val_accuracy</td><td>▁▅▆▅█▆▇</td></tr><tr><td>val_f1_score_macro</td><td>▁▅▆▅█▆▇</td></tr><tr><td>val_loss</td><td>█▃▃▄▁▃▂</td></tr><tr><td>val_precision_macro</td><td>▁▅▅▅█▅█</td></tr><tr><td>val_recall_macro</td><td>▁▅▅▅█▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.99324</td></tr><tr><td>train_f1_score_macro</td><td>0.99323</td></tr><tr><td>train_loss</td><td>0.02068</td></tr><tr><td>train_precision_macro</td><td>0.99324</td></tr><tr><td>train_recall_macro</td><td>0.99321</td></tr><tr><td>val_accuracy</td><td>0.98867</td></tr><tr><td>val_classification_report_path</td><td>classification_repor...</td></tr><tr><td>val_f1_score_macro</td><td>0.98865</td></tr><tr><td>val_loss</td><td>0.03658</td></tr><tr><td>val_precision_macro</td><td>0.99</td></tr><tr><td>val_recall_macro</td><td>0.99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1337-check-reproducibility</strong> at: <a href='https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final/runs/9rn6etf8' target=\"_blank\">https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final/runs/9rn6etf8</a><br> View project at: <a href='https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final' target=\"_blank\">https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final</a><br>Synced 5 W&B file(s), 21 media file(s), 28 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250710_224748-9rn6etf8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Yindan\\Documents\\Bamberg4\\xAI_proj\\xAI-code\\xAI-code\\mnist_BL\\wandb\\run-20250710_225707-u3xo53gf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final/runs/u3xo53gf' target=\"_blank\">1337-check-reproducibility2</a></strong> to <a href='https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final' target=\"_blank\">https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final/runs/u3xo53gf' target=\"_blank\">https://wandb.ai/mayindan-university-of-bamberg/mnist-baseline-final/runs/u3xo53gf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices (first 10): [27415, 57082, 25212, 22657, 23939, 28148, 28313, 3041, 10690, 18767]\n",
      "Val indices (first 10): [6991, 39897, 3982, 52646, 14149, 38675, 18106, 42669, 33438, 48256]\n",
      "[Epoch 1] Train Loss: 0.2998, Train Acc: 0.9148, Train Precision: 0.9152, Train Recall: 0.9138, Train F1: 0.9142\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       1.00    0.89      0.94     1442  0.893897\n",
      "           1       0.98    0.98      0.98     1672  0.982656\n",
      "           2       0.97    0.98      0.97     1504  0.978059\n",
      "           3       0.98    0.98      0.98     1500  0.983333\n",
      "           4       0.98    0.98      0.98     1460  0.976027\n",
      "           5       0.98    0.99      0.98     1326  0.987179\n",
      "           6       0.98    0.99      0.98     1486  0.986541\n",
      "           7       0.99    0.95      0.97     1624  0.946429\n",
      "           8       0.89    0.98      0.93     1473  0.983707\n",
      "           9       0.96    0.98      0.97     1513  0.977528\n",
      "   macro avg       0.97    0.97      0.97    15000       NaN\n",
      "weighted avg       0.97    0.97      0.97    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.969533\n",
      "Best model saved with val accuracy: 0.9695\n",
      "[Epoch 2] Train Loss: 0.0544, Train Acc: 0.9832, Train Precision: 0.9831, Train Recall: 0.9831, Train F1: 0.9831\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       1.00    0.99      0.99     1442  0.985437\n",
      "           1       0.98    1.00      0.99     1672  0.995215\n",
      "           2       1.00    0.95      0.97     1504  0.951463\n",
      "           3       0.99    0.98      0.99     1500  0.984667\n",
      "           4       0.97    1.00      0.98     1460  0.995205\n",
      "           5       0.99    0.98      0.99     1326  0.979638\n",
      "           6       1.00    0.98      0.99     1486  0.979812\n",
      "           7       0.95    1.00      0.97     1624  0.995074\n",
      "           8       0.96    0.99      0.98     1473  0.986422\n",
      "           9       0.99    0.97      0.98     1513  0.969597\n",
      "   macro avg       0.98    0.98      0.98    15000       NaN\n",
      "weighted avg       0.98    0.98      0.98    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.982467\n",
      "Best model saved with val accuracy: 0.9825\n",
      "[Epoch 3] Train Loss: 0.0391, Train Acc: 0.9882, Train Precision: 0.9881, Train Recall: 0.9881, Train F1: 0.9881\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       1.00    0.96      0.98     1442  0.958391\n",
      "           1       0.99    0.99      0.99     1672  0.994019\n",
      "           2       0.98    0.99      0.99     1504  0.992686\n",
      "           3       0.97    1.00      0.98     1500  0.998000\n",
      "           4       0.98    0.99      0.99     1460  0.989726\n",
      "           5       1.00    0.96      0.98     1326  0.961538\n",
      "           6       0.97    1.00      0.98     1486  0.995962\n",
      "           7       1.00    0.98      0.99     1624  0.982759\n",
      "           8       0.98    0.98      0.98     1473  0.981670\n",
      "           9       0.98    0.98      0.98     1513  0.981494\n",
      "   macro avg       0.98    0.98      0.98    15000       NaN\n",
      "weighted avg       0.98    0.98      0.98    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.984067\n",
      "Best model saved with val accuracy: 0.9841\n",
      "[Epoch 4] Train Loss: 0.0301, Train Acc: 0.9904, Train Precision: 0.9903, Train Recall: 0.9903, Train F1: 0.9903\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       0.97    0.99      0.98     1442  0.994452\n",
      "           1       0.99    0.99      0.99     1672  0.992823\n",
      "           2       0.97    0.99      0.98     1504  0.994681\n",
      "           3       0.96    0.99      0.97     1500  0.988667\n",
      "           4       1.00    0.98      0.99     1460  0.977397\n",
      "           5       0.98    0.98      0.98     1326  0.982655\n",
      "           6       1.00    0.97      0.98     1486  0.965680\n",
      "           7       0.97    1.00      0.98     1624  0.995074\n",
      "           8       0.99    0.98      0.99     1473  0.980312\n",
      "           9       0.99    0.95      0.97     1513  0.946464\n",
      "   macro avg       0.98    0.98      0.98    15000       NaN\n",
      "weighted avg       0.98    0.98      0.98    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.982000\n",
      "[Epoch 5] Train Loss: 0.0263, Train Acc: 0.9920, Train Precision: 0.9919, Train Recall: 0.9919, Train F1: 0.9919\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       0.99    0.99      0.99     1442  0.994452\n",
      "           1       1.00    0.99      0.99     1672  0.992225\n",
      "           2       0.99    0.99      0.99     1504  0.992021\n",
      "           3       0.99    0.99      0.99     1500  0.992667\n",
      "           4       0.99    0.99      0.99     1460  0.993151\n",
      "           5       0.99    0.99      0.99     1326  0.990196\n",
      "           6       0.99    1.00      0.99     1486  0.995289\n",
      "           7       0.98    1.00      0.99     1624  0.996921\n",
      "           8       1.00    0.98      0.99     1473  0.984386\n",
      "           9       0.99    0.98      0.99     1513  0.978850\n",
      "   macro avg       0.99    0.99      0.99    15000       NaN\n",
      "weighted avg       0.99    0.99      0.99    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.991067\n",
      "Best model saved with val accuracy: 0.9911\n",
      "[Epoch 6] Train Loss: 0.0197, Train Acc: 0.9938, Train Precision: 0.9938, Train Recall: 0.9938, Train F1: 0.9938\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       0.99    0.98      0.99     1442  0.981969\n",
      "           1       0.99    1.00      0.99     1672  0.995215\n",
      "           2       0.99    0.99      0.99     1504  0.991356\n",
      "           3       1.00    0.98      0.99     1500  0.983333\n",
      "           4       0.95    0.99      0.97     1460  0.989726\n",
      "           5       0.99    0.99      0.99     1326  0.990196\n",
      "           6       0.96    1.00      0.98     1486  0.998654\n",
      "           7       0.98    0.99      0.98     1624  0.990764\n",
      "           8       0.99    0.98      0.99     1473  0.983028\n",
      "           9       1.00    0.93      0.97     1513  0.933906\n",
      "   macro avg       0.98    0.98      0.98    15000       NaN\n",
      "weighted avg       0.98    0.98      0.98    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.983867\n",
      "[Epoch 7] Train Loss: 0.0207, Train Acc: 0.9932, Train Precision: 0.9932, Train Recall: 0.9932, Train F1: 0.9932\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       1.00    0.98      0.99     1442  0.984743\n",
      "           1       0.98    1.00      0.99     1672  0.997010\n",
      "           2       0.99    0.98      0.99     1504  0.983378\n",
      "           3       0.99    0.99      0.99     1500  0.991333\n",
      "           4       0.99    0.98      0.99     1460  0.982877\n",
      "           5       0.98    1.00      0.99     1326  0.996229\n",
      "           6       0.98    0.99      0.99     1486  0.987887\n",
      "           7       0.99    1.00      0.99     1624  0.995074\n",
      "           8       0.99    0.98      0.99     1473  0.980991\n",
      "           9       0.98    0.99      0.99     1513  0.986120\n",
      "   macro avg       0.99    0.99      0.99    15000       NaN\n",
      "weighted avg       0.99    0.99      0.99    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.988667\n",
      "[Epoch 8] Train Loss: 0.0165, Train Acc: 0.9950, Train Precision: 0.9950, Train Recall: 0.9950, Train F1: 0.9950\n",
      "\n",
      " VAL Classification Report:\n",
      "\n",
      "\n",
      "Classification Report Table (with Accuracy):\n",
      "       Class  Precision  Recall  F1-Score  Support  Accuracy\n",
      "           0       1.00    0.99      0.99     1442  0.988211\n",
      "           1       1.00    0.98      0.99     1672  0.982656\n",
      "           2       0.98    0.99      0.99     1504  0.992021\n",
      "           3       0.98    1.00      0.99     1500  0.996667\n",
      "           4       0.99    0.99      0.99     1460  0.991781\n",
      "           5       1.00    0.99      1.00     1326  0.993967\n",
      "           6       0.99    0.99      0.99     1486  0.990579\n",
      "           7       0.99    0.98      0.98     1624  0.975985\n",
      "           8       0.98    0.99      0.99     1473  0.991174\n",
      "           9       0.98    0.99      0.99     1513  0.988764\n",
      "   macro avg       0.99    0.99      0.99    15000       NaN\n",
      "weighted avg       0.99    0.99      0.99    15000       NaN\n",
      "    accuracy        NaN     NaN       NaN    15000  0.988933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 235\u001b[0m\n\u001b[0;32m    232\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    233\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 235\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    238\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"mnist-baseline-final\", \n",
    "           name=\"1337\",\n",
    "           config={\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"model_name\": \"resnet18\",\n",
    "    \"seed\": 1337\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Set random seed\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Use seeded generator for reproducibility\n",
    "generator = torch.Generator().manual_seed(config.seed)\n",
    "\n",
    "\n",
    "# Load MNIST training set (60,000 images)\n",
    "train_val_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Split the training set into 75% train and 25% val \n",
    "train_size = int(0.75 * len(train_val_dataset))  # 45,000\n",
    "val_size = len(train_val_dataset) - train_size   # 15,000\n",
    "train_set, val_set = random_split(train_val_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# Check reproducibility: Print the indices\n",
    "train_indices = train_set.indices if hasattr(train_set, \"indices\") else None\n",
    "val_indices = val_set.indices if hasattr(val_set, \"indices\") else None\n",
    "\n",
    "print(\"Train indices (first 10):\", train_indices[:10])\n",
    "print(\"Val indices (first 10):\", val_indices[:10])\n",
    "\n",
    "# Load the official MNIST test set (10,000 images)\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True, generator=generator)\n",
    "val_loader = DataLoader(val_set, batch_size=config.batch_size, shuffle=False, generator=generator)\n",
    "test_loader = DataLoader(test_set, batch_size=config.batch_size, shuffle=False, generator=generator)\n",
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = timm.create_model(config.model_name, pretrained=False, num_classes=10, in_chans=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "class_names = [str(i) for i in range(10)]\n",
    "\n",
    "# define evaluation function for val and test\n",
    "\n",
    "def evaluate_model(\n",
    "    model,\n",
    "    data_loader,\n",
    "    criterion,\n",
    "    class_names,\n",
    "    mode=\"val\",        # \"val\" or \"test\"\n",
    "    epoch=None,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0.0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "    per_class_acc = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "\n",
    "    # === Classification report ===\n",
    "    report_text = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "    print(f\"\\n {mode.upper()} Classification Report:\\n\")\n",
    "    #print(report_text)\n",
    "\n",
    "    if epoch is not None:\n",
    "        report_path = f\"classification_report_{mode}_epoch{epoch+1}.txt\"\n",
    "    else:\n",
    "        report_path = f\"classification_report_{mode}.txt\"\n",
    "\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    # === Confusion matrix plot ===\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f\"{mode.capitalize()} Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # === Per-class accuracy table ===\n",
    "    acc_table = wandb.Table(columns=[\"Class\", \"Accuracy\"])\n",
    "    for cls, acc in zip(class_names, per_class_acc):\n",
    "        acc_table.add_data(cls, float(acc))\n",
    "\n",
    "    # === Classification report as W&B Table with accuracy column ===\n",
    "    report_df = pd.read_fwf(StringIO(report_text), index_col=0)\n",
    "    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\", \"Accuracy\"])\n",
    "\n",
    "    rows_to_log = []\n",
    "    accuracy_row = None\n",
    "\n",
    "    for idx, row in report_df.iterrows():\n",
    "        precision = row.get(\"precision\", None)\n",
    "        recall = row.get(\"recall\", None)\n",
    "        f1_ = row.get(\"f1-score\", None)\n",
    "        support = row.get(\"support\", None)\n",
    "\n",
    "        try:\n",
    "            precision = float(precision) if precision != \"-\" else None\n",
    "            recall = float(recall) if recall != \"-\" else None\n",
    "            f1_ = float(f1_) if f1_ != \"-\" else None\n",
    "            support = int(support) if pd.notna(support) else None\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if idx.isdigit():  # class label\n",
    "            acc = float(per_class_acc[int(idx)])\n",
    "            rows_to_log.append([idx, precision, recall, f1_, support, acc])\n",
    "        elif idx.lower() == \"accuracy\":\n",
    "            # put final accuracy value in \"Accuracy\" column\n",
    "            accuracy_row = [idx, None, None, None, support, accuracy]\n",
    "        else:\n",
    "            rows_to_log.append([idx, precision, recall, f1_, support, None])\n",
    "\n",
    "    for row in rows_to_log:\n",
    "        report_table.add_data(*row)\n",
    "    if accuracy_row:\n",
    "        report_table.add_data(*accuracy_row)\n",
    "\n",
    "    # preprint updated classification report table with accuracy\n",
    "    columns = [\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\", \"Accuracy\"]\n",
    "    df = pd.DataFrame(rows_to_log + ([accuracy_row] if accuracy_row else []), columns=columns)\n",
    "\n",
    "    print(\"\\nClassification Report Table (with Accuracy):\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "\n",
    "    # === W&B logging ===\n",
    "    log_data = {\n",
    "        f\"{mode}_loss\": avg_loss,\n",
    "        f\"{mode}_accuracy\": accuracy,\n",
    "        f\"{mode}_precision_macro\": precision,\n",
    "        f\"{mode}_recall_macro\": recall,\n",
    "        f\"{mode}_f1_score_macro\": f1,\n",
    "        f\"{mode}_classification_report_path\": report_path\n",
    "    }\n",
    "\n",
    "    if epoch is not None:\n",
    "        log_data[\"epoch\"] = epoch + 1\n",
    "        log_data[f\"{mode}_confusion_matrix_image_epoch_{epoch+1}\"] = wandb.Image(fig)\n",
    "        log_data[f\"{mode}_per_class_accuracy_table_epoch_{epoch+1}\"] = acc_table\n",
    "        log_data[f\"{mode}_classification_report_table_epoch_{epoch+1}\"] = report_table\n",
    "    else:\n",
    "        log_data[f\"{mode}_confusion_matrix_image\"] = wandb.Image(fig)\n",
    "        log_data[f\"{mode}_per_class_accuracy_table\"] = acc_table\n",
    "        log_data[f\"{mode}_classification_report_table\"] = report_table\n",
    "\n",
    "    wandb.log(log_data)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Training + Evaluation loop\n",
    "best_val_accuracy = 0.0\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    train_preds, train_labels = [], []\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_accuracy = correct / len(train_loader.dataset)\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_precision = precision_score(train_labels, train_preds, average='macro')\n",
    "    train_recall = recall_score(train_labels, train_preds, average='macro')\n",
    "    train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "        f\"Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"train_precision_macro\": train_precision,\n",
    "        \"train_recall_macro\":train_recall,\n",
    "        \"train_f1_score_macro\": train_f1\n",
    "    })\n",
    "\n",
    "    # validation\n",
    "    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, class_names, mode=\"val\", epoch=epoch)\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"Best model saved with val accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# final test\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.to(device)\n",
    "evaluate_model(model, test_loader, criterion, class_names, mode=\"test\", epoch=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
